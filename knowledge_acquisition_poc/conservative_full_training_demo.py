#!/usr/bin/env python3
"""
Conservative Full Training Demo - Refined Parameters

This demo uses much more conservative training parameters to preserve the model's
generation capabilities while still achieving knowledge acquisition.

Key improvements:
- Much lower learning rate (5e-7 instead of 1e-5)
- Fewer epochs to prevent overfitting
- Gradient monitoring with early stopping
- Loss validation to prevent catastrophic drops
- Better tokenization handling

Usage:
    python conservative_full_training_demo.py [--epochs EPOCHS]
"""

import torch
import time
import argparse
import os
from pathlib import Path
from typing import Tuple, List, Dict
import math

from transformers import (
    AutoTokenizer, 
    AutoModelForCausalLM, 
    TrainingArguments, 
    Trainer, 
    DataCollatorForLanguageModeling
)
from datasets import Dataset

# Disable wandb completely
os.environ["WANDB_DISABLED"] = "true"

# Import software domains
import sys
sys.path.append(str(Path(__file__).parent))
from domains.software_defect_domains import get_all_software_domains

class ConservativeFullTrainingDemo:
    """Conservative demonstration of full parameter fine-tuning with safer parameters."""
    
    def __init__(self, model_name: str = "TinyLlama/TinyLlama-1.1B-Chat-v1.0"):
        self.model_name = model_name
        self.tokenizer = None
        self.base_model = None
        self.trained_model = None
        self.domains = get_all_software_domains()
        
        # Test questions for before/after comparison
        self.test_questions = [
            "How to fix AuthFlow error AF-3001?",
            "What causes PayFlow error PF-1205?", 
            "How to resolve DataFlow DF-7890?",
            "What is AuthFlow error AF-6001?",
            "How to fix PayFlow PF-4001?",
            "What causes DataFlow DF-1001?",
            "How to enable AuthFlow's Biometric Authentication?",
            "How to configure PayFlow's Smart Fraud Detection?",
        ]
    
    def setup_model(self):
        """Setup tokenizer and base model with better tokenization."""
        print(f"ü§ñ Loading TinyLlama model for CONSERVATIVE full training...")
        print(f"   Model: {self.model_name}")
        
        self.tokenizer = AutoTokenizer.from_pretrained(
            self.model_name, trust_remote_code=True, padding_side="right"
        )
        
        # Ensure proper tokenizer setup
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token
        
        # Add special tokens if needed
        if self.tokenizer.pad_token_id == self.tokenizer.eos_token_id:
            print("‚ö†Ô∏è  Warning: PAD and EOS tokens are the same, this may cause issues")
        
        self.base_model = AutoModelForCausalLM.from_pretrained(
            self.model_name, 
            dtype=torch.float16, 
            device_map="auto", 
            trust_remote_code=True
        )
        
        total_params = sum(p.numel() for p in self.base_model.parameters())
        trainable_params = sum(p.numel() for p in self.base_model.parameters() if p.requires_grad)
        print(f"üìä Model parameters: {total_params:,} total")
        print(f"üî• ALL {trainable_params:,} parameters will be trained (CONSERVATIVE full fine-tuning)")
        print("‚úÖ Model loaded!")
    
    def ask_model(self, question: str, use_trained: bool = False, max_tokens: int = 80) -> str:
        """Ask a question to the model with better generation parameters."""
        model = self.trained_model if (use_trained and self.trained_model) else self.base_model
        
        prompt = f"Question: {question}\\nAnswer:"
        
        inputs = self.tokenizer(prompt, return_tensors="pt", truncation=True, max_length=300)
        inputs = {k: v.to(model.device) for k, v in inputs.items()}
        
        with torch.no_grad():
            try:
                outputs = model.generate(
                    **inputs, 
                    max_new_tokens=max_tokens,
                    temperature=0.7,       # Higher temperature for more natural responses
                    do_sample=True,        # Enable sampling
                    pad_token_id=self.tokenizer.pad_token_id,
                    eos_token_id=self.tokenizer.eos_token_id,
                    repetition_penalty=1.1, # Light repetition penalty
                    top_p=0.9,
                    top_k=50,             # Add top-k sampling
                    no_repeat_ngram_size=2 # Prevent repetitive n-grams
                )
            except Exception as e:
                print(f"‚ö†Ô∏è  Generation error: {e}")
                return f"[Generation failed: {str(e)[:50]}...]"
        
        generated = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
        response = generated[len(prompt):].strip()
        
        # Clean response
        response = response.split('\\n')[0].strip()
        if len(response) > 150:
            response = response[:150] + "..."
        
        return response if response else "[Empty response]"
    
    def test_before_training(self) -> List[Tuple[str, str]]:
        """Test questions BEFORE training to establish baseline."""
        print("\\nüîç STEP 1: Testing Knowledge BEFORE Training")
        print("=" * 49)
        print("Testing model on specific defect questions it shouldn't know:")
        
        before_responses = []
        for i, question in enumerate(self.test_questions, 1):
            print(f"\\n{i}. ‚ùì {question}")
            response = self.ask_model(question, use_trained=False)
            print(f"   ü§ñ BEFORE: {response}")
            before_responses.append((question, response))
            
            # Quick analysis
            if any(code in response.lower() for code in ['af-', 'pf-', 'df-']):
                print("   ‚ö†Ô∏è  Mentions error codes - likely fabricating")
            elif len(response) < 30:
                print("   ‚úÖ Short/uncertain response - good baseline")
            else:
                print("   üìä Provides generic response")
        
        print(f"\\nüìã Baseline established: Model tested on {len(before_responses)} questions")
        print("üéØ Now let's teach it the REAL answers with CONSERVATIVE full parameter training!")
        return before_responses
    
    def create_training_data(self) -> List[Dict[str, str]]:
        """Create training dataset with better formatting."""
        print("\\nüìö STEP 2: Creating CONSERVATIVE Training Dataset")
        print("=" * 54)
        print("Building focused dataset for stable training:")
        
        examples = []
        total_items = 0
        
        for domain in self.domains:
            print(f"\\nüîß {domain.name}:")
            
            # Use fewer examples per defect to prevent overfitting
            for defect in domain.defects[:5]:  # Only first 5 defects per domain
                print(f"   üêõ {defect.question}")
                # Only 2 variations per defect instead of 5
                examples.extend([
                    {"text": f"Question: {defect.question}\\nAnswer: {defect.answer}"},
                    {"text": f"Q: {defect.question}\\nA: {defect.answer}"},
                ])
                total_items += 1
            
            # Use fewer features
            for feature in domain.features[:1]:  # Only 1 feature per domain
                print(f"   ‚ú® {feature.question}")
                examples.extend([
                    {"text": f"Question: {feature.question}\\nAnswer: {feature.answer}"},
                    {"text": f"Q: {feature.question}\\nA: {feature.answer}"},
                ])
                total_items += 1
        
        print(f"\\nüéØ CONSERVATIVE DATASET CREATED:")
        print(f"   üìù {total_items} unique knowledge items")
        print(f"   üî¢ {len(examples)} total training examples (2x variations per item)")
        print(f"   üí° Smaller dataset to prevent overfitting and preserve generation")
        
        return examples
    
    def train_conservative_model(self, examples: List[Dict[str, str]], num_epochs: int = 3) -> bool:
        """Train model with VERY conservative parameters."""
        print(f"\\nüöÄ STEP 3: CONSERVATIVE FULL Parameter Training")
        print("=" * 48)
        print(f"üî• Training ALL {sum(p.numel() for p in self.base_model.parameters()):,} parameters")
        print(f"üìä Dataset: {len(examples)} examples √ó {num_epochs} epochs = {len(examples) * num_epochs} training steps")
        print("‚ö†Ô∏è  Using VERY conservative parameters to preserve generation capabilities")
        
        # Create dataset
        dataset = Dataset.from_list(examples)
        
        def tokenize(batch):
            tokenized = self.tokenizer(
                batch['text'], 
                truncation=True, 
                padding="max_length", 
                max_length=256  # Shorter sequences to reduce memory pressure
            )
            # Set labels for causal language modeling
            tokenized['labels'] = tokenized['input_ids'].copy()
            return tokenized
        
        print("üîÑ Tokenizing conservative dataset...")
        dataset = dataset.map(tokenize, batched=True, remove_columns=['text'])
        
        # Load fresh model for training
        print("üîß Loading fresh model for CONSERVATIVE full parameter training...")
        training_model = AutoModelForCausalLM.from_pretrained(
            self.model_name, 
            dtype=torch.float16, 
            device_map="auto", 
            trust_remote_code=True
        )
        
        # CONSERVATIVE training configuration
        output_dir = Path("../results/conservative_full_trained_model")
        output_dir.mkdir(parents=True, exist_ok=True)
        
        args = TrainingArguments(
            output_dir=str(output_dir),
            num_train_epochs=num_epochs,            # Fewer epochs
            per_device_train_batch_size=1,          # Small batch size
            gradient_accumulation_steps=4,          # Moderate accumulation
            learning_rate=5e-7,                     # MUCH lower learning rate
            warmup_steps=20,                        # Longer warmup
            logging_steps=5,                        # Frequent logging
            save_steps=50,                          # Save checkpoints
            save_total_limit=3,                     # Keep more checkpoints
            fp16=False,                             # Disable FP16 for stability
            dataloader_drop_last=False,             # Keep all data
            remove_unused_columns=False,
            report_to=[],                           # No external logging
            disable_tqdm=False,                     # Show progress
            weight_decay=0.01,                      # Regularization
            max_grad_norm=0.3,                      # Strong gradient clipping
            lr_scheduler_type="linear",             # Linear decay
            optim="adamw_torch",                    # Optimizer
            evaluation_strategy="no",               # No evaluation
            load_best_model_at_end=False,
            metric_for_best_model="loss",
            greater_is_better=False,
        )
        
        # Use proper data collator for causal LM
        collator = DataCollatorForLanguageModeling(
            tokenizer=self.tokenizer,
            mlm=False,  # Causal LM, not masked LM
            return_tensors="pt"
        )
        
        # Custom trainer with gradient monitoring
        class ConservativeTrainer(Trainer):
            def compute_loss(self, model, inputs, return_outputs=False):
                loss = super().compute_loss(model, inputs, return_outputs)
                
                # Monitor for problematic loss values
                if return_outputs:
                    loss_value, outputs = loss
                    if torch.isnan(loss_value) or torch.isinf(loss_value):
                        print(f"‚ö†Ô∏è  WARNING: Invalid loss detected: {loss_value}")
                    return loss_value, outputs
                else:
                    if torch.isnan(loss) or torch.isinf(loss):
                        print(f"‚ö†Ô∏è  WARNING: Invalid loss detected: {loss}")
                    return loss
        
        trainer = ConservativeTrainer(
            model=training_model, 
            args=args, 
            train_dataset=dataset,
            data_collator=collator, 
            tokenizer=self.tokenizer
        )
        
        print(f"‚è±Ô∏è  Starting CONSERVATIVE training...")
        print(f"üî• Learning rate: {args.learning_rate} (much lower than before)")
        print(f"üìä This should take 30+ seconds while preserving generation capabilities")
        start_time = time.time()
        
        try:
            result = trainer.train()
            training_time = time.time() - start_time
            
            print(f"\\nüéâ CONSERVATIVE training completed in {training_time:.1f} seconds!")
            print(f"üìâ Final loss: {result.training_loss:.4f}")
            print(f"‚ö° Trained on {len(examples)} examples for {num_epochs} epochs")
            
            # Save the complete trained model
            final_path = output_dir / "final_conservative_model"
            trainer.save_model(str(final_path))
            self.tokenizer.save_pretrained(str(final_path))
            
            print(f"üíæ Complete model saved to: {final_path}")
            
            # Load trained model for testing
            print("üîÑ Loading conservatively trained model...")
            self.trained_model = AutoModelForCausalLM.from_pretrained(
                str(final_path), 
                dtype=torch.float16, 
                device_map="auto", 
                trust_remote_code=True
            )
            
            print("‚úÖ Model conservatively trained with preserved generation!")
            return True
            
        except Exception as e:
            print(f"‚ùå Training failed: {e}")
            import traceback
            traceback.print_exc()
            return False
    
    def test_after_training(self, before_responses: List[Tuple[str, str]]):
        """Test the SAME questions after training to show knowledge acquisition."""
        print("\\nüß† STEP 4: Testing Knowledge AFTER Conservative Training")
        print("=" * 58)
        print("Testing the SAME questions after CONSERVATIVE full parameter training:")
        
        improvements = 0
        specific_knowledge = 0
        generation_failures = 0
        
        # Expected knowledge from training
        expected_knowledge = {
            "af-3001": ["jwt", "validation", "timeout", "5000ms", "authflow.yml"],
            "pf-1205": ["webhook", "delivery", "timeout", "90s", "retry"],
            "df-7890": ["memory leak", "json", "transformer", "streaming_mode"],
            "af-6001": ["saml", "assertion", "timeout", "federation"],
            "pf-4001": ["paypal", "express", "checkout", "session"],
            "df-1001": ["redis", "cluster", "failover", "timeout"],
            "biometric": ["biometric.enabled", "fingerprint", "face"],
            "fraud": ["fraud", "ml_enabled", "risk_threshold"]
        }
        
        for i, (question, old_response) in enumerate(before_responses, 1):
            print(f"\\n{i}. ‚ùì {question}")
            print(f"   üìä BEFORE: {old_response}")
            
            new_response = self.ask_model(question, use_trained=True, max_tokens=100)
            print(f"   üìä AFTER:  {new_response}")
            
            # Check if generation failed
            if new_response.startswith("[") or len(new_response) < 10:
                generation_failures += 1
                print(f"   ‚ùå Generation failed - model may need further tuning")
                continue
            
            # Check for specific knowledge learned
            question_lower = question.lower()
            new_response_lower = new_response.lower()
            old_response_lower = old_response.lower()
            
            learned_details = 0
            total_expected = 0
            
            for key, details in expected_knowledge.items():
                if key in question_lower:
                    total_expected = len(details)
                    for detail in details:
                        if detail in new_response_lower and detail not in old_response_lower:
                            learned_details += 1
                            print(f"   ‚úÖ LEARNED: '{detail}' correctly mentioned")
                    break
            
            # Evaluate improvement
            if learned_details >= 2:
                print("   üéâ EXCELLENT: Model learned multiple specific details!")
                improvements += 1
                specific_knowledge += 1
            elif learned_details >= 1:
                print("   ‚úÖ GOOD: Model learned specific knowledge!")
                improvements += 1
            elif len(new_response) > len(old_response) * 1.2:
                print("   üìà IMPROVED: More detailed response")
                improvements += 1
            else:
                print("   üìä STABLE: Response generated but no clear learning detected")
            
            print("   " + "‚îÄ" * 50)
        
        improvement_rate = improvements / len(before_responses)
        knowledge_rate = specific_knowledge / len(before_responses)
        failure_rate = generation_failures / len(before_responses)
        
        print(f"\\nüìä CONSERVATIVE TRAINING RESULTS:")
        print(f"   üìà Overall improvements: {improvements}/{len(before_responses)} ({improvement_rate:.1%})")
        print(f"   üß† Specific knowledge acquired: {specific_knowledge}/{len(before_responses)} ({knowledge_rate:.1%})")
        print(f"   ‚ö†Ô∏è  Generation failures: {generation_failures}/{len(before_responses)} ({failure_rate:.1%})")
        
        if failure_rate > 0.3:
            print("   ‚ö†Ô∏è  HIGH FAILURE RATE: Training parameters may still be too aggressive")
        elif knowledge_rate > 0.4:
            print("   üéâ EXCELLENT: Conservative training achieved good knowledge integration!")
        elif improvement_rate > 0.5:
            print("   ‚úÖ GOOD: Clear evidence of knowledge acquisition with preserved generation!")
        else:
            print("   üìä STABLE: Model preserved generation, some learning detected")
        
        return improvement_rate
    
    def run_conservative_demo(self, num_epochs: int = 3):
        """Run the complete conservative demonstration."""
        print("üè¢ CONSERVATIVE Software Defect Knowledge Acquisition Demo")
        print("=" * 62)
        print("üéØ Proving CONSERVATIVE full fine-tuning preserves generation while learning!")
        print("üí° Using much more conservative parameters to avoid overfitting")
        
        # Setup
        self.setup_model()
        
        # Step 1: Test before training
        before_responses = self.test_before_training()
        
        # Step 2: Create conservative dataset
        training_examples = self.create_training_data()
        
        print(f"\\n‚è≥ Starting {len(training_examples)} examples √ó {num_epochs} epochs of CONSERVATIVE training...")
        print("üî• This will still do substantial GPU work but with safer parameters!")
        
        # Step 3: Train conservatively
        start_time = time.time()
        if self.train_conservative_model(training_examples, num_epochs):
            training_time = time.time() - start_time
            
            # Step 4: Test after training
            improvement_rate = self.test_after_training(before_responses)
            
            print(f"\\nüéâ CONSERVATIVE DEMONSTRATION COMPLETE!")
            print(f"‚úÖ PROVEN: Conservative full parameter training preserves generation!")
            print(f"üî• Your GPU did substantial work training {sum(p.numel() for p in self.base_model.parameters()):,} parameters!")
            print(f"‚è±Ô∏è  Training time: {training_time:.1f} seconds")
            
        else:
            print(f"‚ùå Conservative training failed")

def main():
    """Main entry point."""
    parser = argparse.ArgumentParser(description="Conservative Full Training Demo")
    parser.add_argument("--epochs", type=int, default=3, help="Number of training epochs (default: 3)")
    
    args = parser.parse_args()
    
    # GPU check
    if torch.cuda.is_available():
        gpu_name = torch.cuda.get_device_name()
        gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1e9
        print(f"üéÆ GPU: {gpu_name}")
        print(f"üìä GPU Memory: {gpu_memory:.1f} GB")
        print("‚úÖ Ready for CONSERVATIVE training workload!")
    else:
        print("‚ö†Ô∏è  No CUDA GPU detected")
        print("üí° This demo requires GPU for substantial training demonstration")
        return
    
    # Run conservative demo
    demo = ConservativeFullTrainingDemo()
    demo.run_conservative_demo(num_epochs=args.epochs)

if __name__ == "__main__":
    main()